{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   name  \\\n",
      "0  11August_2010_Wednesday_tagesschau-1   \n",
      "1  11August_2010_Wednesday_tagesschau-4   \n",
      "2  11August_2010_Wednesday_tagesschau-5   \n",
      "3  11August_2010_Wednesday_tagesschau-6   \n",
      "4  11August_2010_Wednesday_tagesschau-7   \n",
      "\n",
      "                                          video  start  end   speaker  \\\n",
      "0  11August_2010_Wednesday_tagesschau-1/1/*.png     -1   -1  Signer08   \n",
      "1  11August_2010_Wednesday_tagesschau-4/1/*.png     -1   -1  Signer08   \n",
      "2  11August_2010_Wednesday_tagesschau-5/1/*.png     -1   -1  Signer08   \n",
      "3  11August_2010_Wednesday_tagesschau-6/1/*.png     -1   -1  Signer08   \n",
      "4  11August_2010_Wednesday_tagesschau-7/1/*.png     -1   -1  Signer08   \n",
      "\n",
      "                                                orth  \\\n",
      "0  __ON__ JETZT WETTER MORGEN DONNERSTAG ZWOELF F...   \n",
      "1  ORT-PLUSPLUS REGEN DURCH REGEN-PLUSPLUS KOENNE...   \n",
      "2  __ON__ loc-NORDWEST HEUTE NACHT TROCKEN BLEIBE...   \n",
      "3  TAGSUEBER OFT REGEN-PLUSPLUS GEWITTER GEWITTER...   \n",
      "4  __ON__ WOLKE LOCH LOCH SPEZIELL loc-NORDWEST _...   \n",
      "\n",
      "                                         translation  \n",
      "0  und nun die wettervorhersage für morgen donner...  \n",
      "1  mancherorts regnet es auch länger und ergiebig...  \n",
      "2  im nordwesten bleibt es heute nacht meist troc...  \n",
      "3  auch am tag gibt es verbreitet zum teil kräfti...  \n",
      "4  größere wolkenlücken finden sich vor allem im ...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1862, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/annotations/manual/PHOENIX-2014-T.train-complex-annotation.corpus.csv'\n",
    "dataframe = pd.read_csv(path, sep='|')\n",
    "print (dataframe.head(5))\n",
    "# Removing start and end columns\n",
    "dataframe.drop(columns=['start', 'end'], inplace=True)\n",
    "\n",
    "# Working on signer 1 alone\n",
    "signer1_dataframe = dataframe[dataframe['speaker']=='Signer01']\n",
    "\n",
    "signer1_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    m = 0\n",
    "    max_folder = ''\n",
    "    list_folders = []\n",
    "    list_len = []\n",
    "\n",
    "    for folder in os.listdir('../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/'):\n",
    "        l = len(os.listdir('../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/' + folder))\n",
    "        list_folders.append(folder)\n",
    "        list_len.append(l)\n",
    "\n",
    "    temp_df = pd.DataFrame(list(zip(list_folders, list_len)), columns=['Folder', 'No. of images'])\n",
    "    \n",
    "    plt.hist(temp_df['No. of images'], bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_data = np.array(signer1_dataframe['orth'])\n",
    "# for num in range (len(loaded_data)):\n",
    "#     sentence = loaded_data[num].split()\n",
    "#     sentence.insert(0, '<sos>')\n",
    "#     space = \" \"\n",
    "#     sentence = space.join(sentence)\n",
    "#     loaded_data[num] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:  __ON__ ITALIEN IX TIEF DRUCK cl-KOMMEN HEUTE NACHT BERG SCHNEE REGEN REGEN __OFF__\n",
      "encoded:  tensor([ 5, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 24, 14,  2,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "decoded:  __ON__ ITALIEN IX TIEF DRUCK cl-KOMMEN HEUTE NACHT BERG SCHNEE REGEN REGEN __OFF__ </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "from torchnlp.encoders.text import StaticTokenizerEncoder, SpacyEncoder, pad_tensor\n",
    "loaded_data = np.array(signer1_dataframe['orth'])\n",
    "encoder = StaticTokenizerEncoder(loaded_data, tokenize=lambda s: s.split(), append_eos=True)\n",
    "\n",
    "encoded_data = [encoder.encode(example) for example in loaded_data]\n",
    "encoded_data = [pad_tensor(x, length=35) for x in encoded_data]\n",
    "\n",
    "example_encode = encoder.encode(loaded_data[1])\n",
    "example_pad = pad_tensor(example_encode, length=35)\n",
    "\n",
    "print('actual: ', loaded_data[1])\n",
    "print('encoded: ',example_pad)\n",
    "print('decoded: ', encoder.decode(example_pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8003e0fe6cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mBOS_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mEOS_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"\"\" Vocabulary represents mapping between tokens and indices. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8003e0fe6cbd>\u001b[0m in \u001b[0;36mVocabulary\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_UNK_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_from_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[1;32m     19\u001b[0m         \u001b[0mMake\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "SIL_TOKEN = \"<si>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\" Vocabulary represents mapping between tokens and indices. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # don't rename stoi and itos since needed for torchtext\n",
    "        # warning: stoi grows with unknown tokens, don't use for saving or size\n",
    "        self.specials = []\n",
    "        self.itos = []\n",
    "        self.stoi = None\n",
    "        self.DEFAULT_UNK_ID = None\n",
    "\n",
    "    def _from_list(self, tokens: List[str] = None):\n",
    "        \"\"\"\n",
    "        Make vocabulary from list of tokens.\n",
    "        Tokens are assumed to be unique and pre-selected.\n",
    "        Special symbols are added if not in list.\n",
    "        :param tokens: list of tokens\n",
    "        \"\"\"\n",
    "        self.add_tokens(tokens=self.specials + tokens)\n",
    "        assert len(self.stoi) == len(self.itos)\n",
    "\n",
    "    def _from_file(self, file: str):\n",
    "        \"\"\"\n",
    "        Make vocabulary from contents of file.\n",
    "        File format: token with index i is in line i.\n",
    "        :param file: path to file where the vocabulary is loaded from\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as open_file:\n",
    "            for line in open_file:\n",
    "                tokens.append(line.strip(\"\\n\"))\n",
    "        self._from_list(tokens)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.stoi.__str__()\n",
    "\n",
    "    def to_file(self, file: str):\n",
    "        \"\"\"\n",
    "        Save the vocabulary to a file, by writing token with index i in line i.\n",
    "        :param file: path to file where the vocabulary is written\n",
    "        \"\"\"\n",
    "        with open(file, \"w\", encoding=\"utf-8\") as open_file:\n",
    "            for t in self.itos:\n",
    "                open_file.write(\"{}\\n\".format(t))\n",
    "\n",
    "    def add_tokens(self, tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add list of tokens to vocabulary\n",
    "        :param tokens: list of tokens to add to the vocabulary\n",
    "        \"\"\"\n",
    "        for t in tokens:\n",
    "            new_index = len(self.itos)\n",
    "            # add to vocab if not already there\n",
    "            if t not in self.itos:\n",
    "                self.itos.append(t)\n",
    "                self.stoi[t] = new_index\n",
    "\n",
    "    def is_unk(self, token: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether a token is covered by the vocabulary\n",
    "        :param token:\n",
    "        :return: True if covered, False otherwise\n",
    "        \"\"\"\n",
    "        return self.stoi[token] == self.DEFAULT_UNK_ID()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.itos)\n",
    "    \n",
    "    \n",
    "\n",
    "class GlossVocabulary(Vocabulary):\n",
    "    def __init__(self, tokens: List[str] = None, file: str = None):\n",
    "        \"\"\"\n",
    "        Create vocabulary from list of tokens or file.\n",
    "        Special tokens are added if not already in file or list.\n",
    "        File format: token with index i is in line i.\n",
    "        :param tokens: list of tokens\n",
    "        :param file: file to load vocabulary from\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.specials = [SIL_TOKEN, UNK_TOKEN, PAD_TOKEN]\n",
    "        self.DEFAULT_UNK_ID = lambda: 1\n",
    "        self.stoi = defaultdict(self.DEFAULT_UNK_ID)\n",
    "\n",
    "        if tokens is not None:\n",
    "            self._from_list(tokens)\n",
    "        elif file is not None:\n",
    "            self._from_file(file)\n",
    "\n",
    "        # TODO (Cihan): This bit is hardcoded so that the silence token\n",
    "        #   is the first label to be able to do CTC calculations (decoding etc.)\n",
    "        #   Might fix in the future.\n",
    "        assert self.stoi[SIL_TOKEN] == 0\n",
    "        \n",
    "    def array_to_sentence(self, array: np.array, cut_at_eos=True) -> List[str]:\n",
    "        \"\"\"\n",
    "        Converts an array of IDs to a sentence, optionally cutting the result\n",
    "        off at the end-of-sequence token.\n",
    "        :param array: 1D array containing indices\n",
    "        :param cut_at_eos: cut the decoded sentences at the first <eos>\n",
    "        :return: list of strings (tokens)\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        for i in array:\n",
    "            s = self.itos[i]\n",
    "            if cut_at_eos and s == EOS_TOKEN:\n",
    "                break\n",
    "            sentence.append(s)\n",
    "        return sentence\n",
    "\n",
    "    def arrays_to_sentences(self, arrays: np.array) -> List[List[str]]:\n",
    "        gloss_sequences = []\n",
    "        for array in arrays:\n",
    "            sequence = []\n",
    "            for i in array:\n",
    "                sequence.append(self.itos[i])\n",
    "            gloss_sequences.append(sequence)\n",
    "        return gloss_sequences\n",
    "\n",
    "\n",
    "def filter_min(counter: Counter, minimum_freq: int):\n",
    "    \"\"\" Filter counter by min frequency \"\"\"\n",
    "    filtered_counter = Counter({t: c for t, c in counter.items() if c >= minimum_freq})\n",
    "    return filtered_counter\n",
    "\n",
    "\n",
    "def sort_and_cut(counter: Counter, limit: int):\n",
    "    \"\"\" Cut counter to most frequent,\n",
    "    sorted numerically and alphabetically\"\"\"\n",
    "    # sort by frequency, then alphabetically\n",
    "    tokens_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "    tokens_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    vocab_tokens = [i[0] for i in tokens_and_frequencies[:limit]]\n",
    "    return vocab_tokens\n",
    "\n",
    "def build_vocab(\n",
    "    field: str, max_size: int, min_freq: int, dataset: Dataset, vocab_file: str = None\n",
    ") -> Vocabulary:\n",
    "    \"\"\"\n",
    "    Builds vocabulary for a torchtext `field` from given`dataset` or\n",
    "    `vocab_file`.\n",
    "    :param field: attribute e.g. \"src\"\n",
    "    :param max_size: maximum size of vocabulary\n",
    "    :param min_freq: minimum frequency for an item to be included\n",
    "    :param dataset: dataset to load data for field from\n",
    "    :param vocab_file: file to store the vocabulary,\n",
    "        if not None, load vocabulary from here\n",
    "    :return: Vocabulary created from either `dataset` or `vocab_file`\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_file is not None:\n",
    "        # load it from file\n",
    "        if field == \"gls\":\n",
    "            vocab = GlossVocabulary(file=vocab_file)\n",
    "        elif field == \"txt\":\n",
    "            vocab = TextVocabulary(file=vocab_file)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown vocabulary type\")\n",
    "    else:\n",
    "        tokens = []\n",
    "        for i in dataset.examples:\n",
    "            if field == \"gls\":\n",
    "                tokens.extend(i.gls)\n",
    "            elif field == \"txt\":\n",
    "                tokens.extend(i.txt)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown field type\")\n",
    "\n",
    "        counter = Counter(tokens)\n",
    "        if min_freq > -1:\n",
    "            counter = filter_min(counter, min_freq)\n",
    "        vocab_tokens = sort_and_cut(counter, max_size)\n",
    "        assert len(vocab_tokens) <= max_size\n",
    "\n",
    "        if field == \"gls\":\n",
    "            vocab = GlossVocabulary(tokens=vocab_tokens)\n",
    "        elif field == \"txt\":\n",
    "            vocab = TextVocabulary(tokens=vocab_tokens)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown vocabulary type\")\n",
    "\n",
    "        assert len(vocab) <= max_size + len(vocab.specials)\n",
    "        assert vocab.itos[vocab.DEFAULT_UNK_ID()] == UNK_TOKEN\n",
    "\n",
    "    for i, s in enumerate(vocab.specials):\n",
    "        if i != vocab.DEFAULT_UNK_ID():\n",
    "            assert not vocab.is_unk(s)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "gls_field = data.Field(\n",
    "        pad_token=PAD_TOKEN,\n",
    "        tokenize=tokenize_text,\n",
    "        batch_first=True,\n",
    "        lower=False,\n",
    "        include_lengths=True)\n",
    "\n",
    "gls_vocab = build_vocab(\n",
    "        field=\"gls\",\n",
    "        min_freq=1,\n",
    "        max_size=1000,\n",
    "        dataset=loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/transformers/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Possible nested set at position 2\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# custom spacy tokenizer\n",
    "\n",
    "# special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "prefix_re = re.compile(r'''^[[(\"']''')\n",
    "suffix_re = re.compile(r'''[])\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                url_match=simple_url_re.match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_ger = spacy.load(\"de\")\n",
    "\n",
    "german = Field(lower=True, init_token=\"<sos>\", eos_token=\"<eos>\", fix_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = np.array(signer1_dataframe['orth'])\n",
    "german.build_vocab(loaded_data, max_size=10000, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fafcdf73950>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             '<sos>': 2,\n",
       "             '<eos>': 3,\n",
       "             ' ': 4,\n",
       "             'E': 5,\n",
       "             'N': 6,\n",
       "             '_': 7,\n",
       "             'O': 8,\n",
       "             'S': 9,\n",
       "             'R': 10,\n",
       "             'I': 11,\n",
       "             'T': 12,\n",
       "             'A': 13,\n",
       "             'H': 14,\n",
       "             'G': 15,\n",
       "             'U': 16,\n",
       "             'D': 17,\n",
       "             'M': 18,\n",
       "             'F': 19,\n",
       "             'L': 20,\n",
       "             'C': 21,\n",
       "             'W': 22,\n",
       "             'K': 23,\n",
       "             'B': 24,\n",
       "             '-': 25,\n",
       "             'P': 26,\n",
       "             'Z': 27,\n",
       "             'l': 28,\n",
       "             'c': 29,\n",
       "             'o': 30,\n",
       "             'V': 31,\n",
       "             'X': 32,\n",
       "             's': 33,\n",
       "             'J': 34,\n",
       "             'p': 35,\n",
       "             '?': 36,\n",
       "             'e': 37,\n",
       "             'g': 38,\n",
       "             'n': 39,\n",
       "             'Y': 40,\n",
       "             'a': 41,\n",
       "             '2': 42,\n",
       "             'Q': 43,\n",
       "             '0': 44,\n",
       "             ':': 45})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor(),normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "signer1_path = '../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/'\n",
    "signer1_main = signer1_dataframe.copy(deep=True)\n",
    "for folder in tqdm(signer1_main['name']):\n",
    "    sequence_length = len(os.listdir(signer1_path + folder))\n",
    "    \n",
    "    if sequence_length > 250 or sequence_length < 50:\n",
    "        signer1_dataframe = signer1_dataframe[signer1_dataframe['name']!=folder]\n",
    "\n",
    "signer1_train, signer1_test = train_test_split(signer1_dataframe, test_size=0.3, random_state=42)\n",
    "signer1_test, signer1_val = train_test_split(signer1_test, test_size=0.5, random_state=42)\n",
    "\n",
    "signer1_train.shape, signer1_test.shape, signer1_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLRT_Signer(Dataset):\n",
    "    \"\"\"SLRT dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_frame, root_dir, transform, tokenizer):\n",
    "        self.images_frame = data_frame['name']\n",
    "        self.glosses = data_frame['orth']\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global device\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        training_example = torch.zeros(250, 3, 224, 224)\n",
    "        for files in os.listdir(os.path.join(self.root_dir, self.images_frame.iloc[idx])):\n",
    "            img_name = self.root_dir + self.images_frame.iloc[idx] + '/' + files\n",
    "            image = Image.open(img_name)\n",
    "            image = self.transform(image)\n",
    "            training_example[0:len(files), :] = image\n",
    "\n",
    "        gloss = self.glosses.iloc[idx]\n",
    "        encoded_gloss = self.tokenizer.encode(gloss)\n",
    "        encoded_gloss = pad_tensor(encoded_gloss, 250)\n",
    "        \n",
    "        return training_example, encoded_gloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signer1_train_dataset = SLRT_Signer(signer1_train,\n",
    "                   root_dir='../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/',\n",
    "                   transform=transform,\n",
    "                   tokenizer=encoder\n",
    "                   )\n",
    "                  \n",
    "signer1_test_dataset = SLRT_Signer(signer1_test,\n",
    "                   root_dir='../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/',\n",
    "                   transform=transform,\n",
    "                   tokenizer=encoder\n",
    "                   )\n",
    "\n",
    "signer1_val_dataset = SLRT_Signer(signer1_val,\n",
    "                   root_dir='../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/',\n",
    "                   transform=transform,\n",
    "                   tokenizer=encoder\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 12,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataLoader(signer1_train_dataset, **params)\n",
    "test_gen = DataLoader(signer1_test_dataset, **params)\n",
    "val_gen = DataLoader(signer1_val_dataset, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, pre_train=False):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        \n",
    "        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'squeezenet1_1', pretrained=True)\n",
    "        \n",
    "        if pre_train == True:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.model.eval()\n",
    "        \n",
    "        children = [child for child in self.model.children()]    \n",
    "        for child in children[0][:-5]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "                \n",
    "        self.fc1 = nn.Linear(1000, 512)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        output = self.model(src)\n",
    "        output = self.fc1(output)\n",
    "        output = F.relu(output)\n",
    "#         output = output.to(torch.device('cpu'))\n",
    "        return output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Testing Squeezenet\n",
    "\n",
    "# path=r'../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/01April_2010_Thursday_heute-6694/images0010.png'\n",
    "\n",
    "# squeeze_model = SqueezeNet().to(device)\n",
    "# summary(squeeze_model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(path)\n",
    "# image = transform(image)\n",
    "# image = image.unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "# output = squeeze_model(image)\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer User Defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "#         self.model_type = 'Transformer'\n",
    "#         self.src_mask = None\n",
    "#         self.tgt_mask = None\n",
    "#         self.nopeakmask = None\n",
    "#         self.pos_encoder = PositionalEncoding(ninp, dropout).to(device)\n",
    "#         encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "#         decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
    "#         self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "#         self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "#         #self.encoder = nn.Embedding(ntoken, ninp)\n",
    "#         self.decoder_embedding = nn.Embedding(ntoken, ninp)\n",
    "#         self.ninp = ninp\n",
    "#         self.decoder = nn.Linear(ninp, ntoken)\n",
    "#         #self.softmax = nn.Softmax(1)\n",
    "        \n",
    "#         #self.init_weights()\n",
    "\n",
    "#     def _generate_square_subsequent_mask(self, src, trt, sz):\n",
    "#         mask = (torch.triu(torch.ones(250, 250)) == 1).transpose(0, 1).half().to(device)\n",
    "#         nopeakmask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).half().to(device)\n",
    "#         zeros = torch.zeros(self.ninp).half().to(device)\n",
    "#         src_msk = (src == zeros).half().to(device)\n",
    "#         target_msk = (trt == 0).unsqueeze(0).half().to(device)\n",
    "#         return src_msk, target_msk, nopeakmask\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.1\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "#     def forward(self, src, trt):\n",
    "# #         trt = trt.to(torch.device('cuda'))\n",
    "#         trt = trt.to(src.device)\n",
    "#         if self.src_mask is None or self.src_mask.size(0) != src.size(0):\n",
    "#             device = src.device\n",
    "#             src_mask, tgt_mask, self.nopeakmask = self._generate_square_subsequent_mask(src, trt, src.size(1))\n",
    "#             self.src_mask = src_mask\n",
    "#             #self.tgt_mask = tgt_mask\n",
    "#         #src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "#         src = self.pos_encoder(src)\n",
    "# #         print (\"source\",src.shape)\n",
    "#         src = src.permute(1,0,2)\n",
    "# #         print (\"source\",src.shape)\n",
    "#         output = self.transformer_encoder(src)\n",
    "# #         trt = trt.to(torch.cuda.device('cpu'))\n",
    "# #         trt = torch.cuda.LongTensor(trt)\n",
    "#         trt = trt.type(torch.cuda.LongTensor)\n",
    "        \n",
    "#         trgt = self.decoder_embedding(trt)\n",
    "#         trgt = self.pos_encoder(trgt)\n",
    "#         trgt = trgt.permute(1,0,2)\n",
    "# #         trgt = trgt[:-1, :]\n",
    "# #         print (trgt.shape), print (output.shape)\n",
    "#         output = self.transformer_decoder(trgt, output, tgt_mask = self.nopeakmask) #tgt_key_padding_mask = tgt_mask)\n",
    "#         output = self.decoder(output)\n",
    "# #         print (output.shape)  shape: 250, batchsize, 719\n",
    "#         output = output.permute(1,2,0)\n",
    "#         #output = output.reshape(-1, output.shape[2])\n",
    "# #         output = self.softmax(output)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Orginal Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelOriginal(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder_layers, num_decoder_layers,\n",
    "        forward_expansion, dropout, max_len, device):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        \n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(embedding_size, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout)\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        \n",
    "#         src_positions = (torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N).to(self.device))\n",
    "        \n",
    "        trg_positions = (torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N).to(self.device))\n",
    "\n",
    "#         embed_src = self.dropout((self.src_word_embedding(src) + self.src_position_embedding(src_positions)))\n",
    "        \n",
    "        embed_trg = self.dropout((self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions)))\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n",
    "\n",
    "        out = self.transformer(embed_src, embed_trg, src_key_padding_mask=src_padding_mask,tgt_mask=trg_mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(encoder.vocab) # the size of vocabulary\n",
    "emsize = 512 # embedding dimension\n",
    "nhid = 1024 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.3 # the dropout value\n",
    "src_pad_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign2Gloss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sign2Gloss_Model(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super (Sign2Gloss_Model, self).__init__()\n",
    "        self.TransformerModel = TransformerModel(ntokens, ninp, nhead, nhid, nlayers, \\\n",
    "                                                 dropout).half()\n",
    "        self.SqueezeNet = SqueezeNet(pre_train=False).half().to(device)\n",
    "        self.softmax = nn.Softmax(2)\n",
    "        self.ninp = ninp\n",
    "    \n",
    "    def forward(self, src, trt):\n",
    "        batch_vector = torch.Tensor(src.shape[0], src.shape[1], self.ninp).half().to(device)\n",
    "        batch_batch_size = 32\n",
    "        for i in range (src.shape[0]):\n",
    "            for batch in range (0, 250, batch_batch_size):\n",
    "                inp = src[i, batch:batch+batch_batch_size].half().to(device)\n",
    "                transformer_source = self.SqueezeNet(inp)\n",
    "                batch_vector[i, batch:batch+batch_batch_size]=transformer_source\n",
    "                del transformer_source, inp\n",
    "                gc.collect\n",
    "#             batch_vector[i] = transformer_source \n",
    "#         batch_vector = batch_vector.half().to(device)\n",
    "        self.TransformerModel = self.TransformerModel.to(device)\n",
    "        output = self.TransformerModel(batch_vector, trt)\n",
    "        output = self.softmax(output)\n",
    "        del batch_vector\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model  = TransformerModel(ntoken=ntokens, ninp=emsize, nhid=nhid, nhead=nhead, nlayers=nlayers, dropout=dropout).to(device)\n",
    "\n",
    "# summary(eval_model, [(-1,250, 512), (8, 250,)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model training\n",
    "# model = Sign2Gloss_Model(ntoken = ntokens, ninp = emsize, nhid = nhid, nlayers = nlayers, nhead = nhead, dropout = dropout)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index = src_pad_index)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "#  # enumerate epochs\n",
    "# for epoch in tqdm(range(30)):\n",
    "# # enumerate mini batches\n",
    "#     epoch_loss = 0\n",
    "#     btch=1\n",
    "# #     print (\"Epoch: \", epoch, \"in progress...\")\n",
    "#     for (inputs, targets) in train_gen:\n",
    "#         print (btch, end=' ')\n",
    "# #         inputs, targets = inputs, targets\n",
    "#         # clear the gradients\n",
    "#         optimizer.zero_grad()\n",
    "#         # compute the model output\n",
    "#         yhat = model(inputs, targets)\n",
    "# #         model = model.to(device)\n",
    "#         yhat = yhat.to(torch.device('cpu'))\n",
    "#         yhat = yhat.type(torch.Tensor)\n",
    "#         targets = targets.type(torch.LongTensor)\n",
    "#         # calculate loss\n",
    "#         loss = criterion(yhat, targets)\n",
    "#         # credit assignment\n",
    "#         loss.backward()\n",
    "#         # update model weights\n",
    "#         optimizer.step()\n",
    "# #         model = model.to(torch.device('cpu'))\n",
    "#         epoch_loss += loss.item()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         del inputs, targets\n",
    "#         gc.collect()\n",
    "#         btch+=1\n",
    "    \n",
    "#     torch.save({\n",
    "#     'epoch': epoch,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'loss': 100\n",
    "#     }, 'squeezenetmode2.pt')\n",
    "    \n",
    "#     try:\n",
    "#         with open('SqueezeNet.txt', 'at') as file:\n",
    "#             now = datetime.now()\n",
    "#             current_time = now.strftime(\"%H:%M:%S\")\n",
    "#             file.write(\"Epoch {}, Logs:{}, Time: {}\\n\".format(epoch, epoch_loss, current_time))\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     print (\"Epoch {} : {}\".format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "# for obj in gc.get_objects():\n",
    "#     try:\n",
    "#         if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "#             print(type(obj), obj.size())\n",
    "#     except:\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#             'epoch': 10,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': 100\n",
    "#             }, 'squeezenetmodel-2_backup.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
